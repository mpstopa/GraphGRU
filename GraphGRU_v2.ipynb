{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGRU(keras.layers.Layer):\n",
    "    def __init__(self, neurons=32, nodesN=10):\n",
    "        super(GraphGRU, self).__init__()\n",
    "        wrgg_init = tf.random_normal_initializer()\n",
    "        self.wrgg = tf.Variable(\n",
    "            initial_value=wrgg_init(shape=(neurons,nodesN,nodesN), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        wrr_init = tf.random_normal_initializer()\n",
    "        self.wrr = tf.Variable(\n",
    "            initial_value=wrr_init(shape=(neurons,neurons), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        br_init = tf.zeros_initializer()\n",
    "        self.br = tf.Variable(\n",
    "            initial_value=br_init(shape=(neurons,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "        wzgg_init = tf.random_normal_initializer()\n",
    "        self.wzgg = tf.Variable(\n",
    "            initial_value=wzgg_init(shape=(neurons,nodesN,nodesN), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        wzz_init = tf.random_normal_initializer()\n",
    "        self.wzz = tf.Variable(\n",
    "            initial_value=wzz_init(shape=(neurons,neurons), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        bz_init = tf.zeros_initializer()\n",
    "        self.bz = tf.Variable(\n",
    "            initial_value=bz_init(shape=(neurons,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "        whgg_init = tf.random_normal_initializer()\n",
    "        self.whgg = tf.Variable(\n",
    "            initial_value=whgg_init(shape=(neurons,nodesN,nodesN), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        whh_init = tf.random_normal_initializer()\n",
    "        self.whh = tf.Variable(\n",
    "            initial_value=whh_init(shape=(neurons,neurons), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        bh_init = tf.zeros_initializer()\n",
    "        self.bh = tf.Variable(\n",
    "            initial_value=bh_init(shape=(neurons,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self,inputs):\n",
    "# inputs is a list of tensors. It seems that I can't have two arguments\n",
    "# to a call to a layer (maybe? in any case get an error when I try). So\n",
    "# one argument as a list of tensors [memory_in,graph] seems to workaround.\n",
    "        memory_in=inputs[0]\n",
    "        graph=inputs[1]\n",
    "        print('graph.shape',graph.shape)\n",
    "        print('memory_in.shape',memory_in.shape)\n",
    "        memory_in=tf.reshape(memory_in,[memory_in[0],1])\n",
    "        Rr=tf.reduce_sum(tf.math.multiply(self.wrgg[:],graph),[1,2])\n",
    "        Rh=tf.linalg.matmul(self.wrr,memory_in)\n",
    "        R=tf.math.sigmoid(Rr+Rh+self.br)\n",
    "        \n",
    "        Zz=tf.reduce_sum(tf.math.multiply(self.wzgg[:],graph),[1,2])\n",
    "        Zh=tf.linalg.matmul(self.wzz,memory_in)\n",
    "        Z=tf.math.sigmoid(Zz+Zh+self.bz)\n",
    "        \n",
    "        hh1=tf.math.multiply(R,memory_in)\n",
    "        H01=tf.reduce_sum(tf.math.multiply(self.whgg[:],graph),[1,2])\n",
    "        H02=tf.linalg.matmul(self.whh,hh1)\n",
    "        H0=tf.math.tanh(H01+H02+self.bh)\n",
    "        \n",
    "        H=tf.math.multiply(Z,memory_in)+tf.math.multiply((1-Z),H0)\n",
    "        \n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.   2.   3. ]\n",
      "  [ 4.   5.   6. ]\n",
      "  [ 7.   8.   9. ]]\n",
      "\n",
      " [[ 0.2  0.4  0.6]\n",
      "  [-1.  -2.  -3. ]\n",
      "  [ 3.   4.   5. ]]] \n",
      " y \n",
      " [[2.2 3.  4. ]\n",
      " [9.  8.  7. ]\n",
      " [1.  2.  3. ]] \n",
      " z \n",
      " [[[  2.2          6.          12.        ]\n",
      "  [ 36.          40.          42.        ]\n",
      "  [  7.          16.          27.        ]]\n",
      "\n",
      " [[  0.44000003   1.2          2.4       ]\n",
      "  [ -9.         -16.         -21.        ]\n",
      "  [  3.           8.          15.        ]]] \n",
      " w \n",
      " [[ 2.64  7.2  14.4 ]\n",
      " [27.   24.   21.  ]\n",
      " [10.   24.   42.  ]] v [[ 4.4  6.   8. ]\n",
      " [18.  16.  14. ]\n",
      " [ 2.   4.   6. ]]\n",
      "WARNING:tensorflow:From C:\\Users\\MStopa\\AppData\\Local\\Continuum\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "graph.shape (15, 15)\n",
      "memory_in.shape (25,)\n",
      "Tensor(\"graph_gru_1/add_6:0\", shape=(25, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x=tf.constant(([[[1,2,3],[4,5,6],[7,8,9]],[[0.2,0.4,0.6],[-1,-2,-3],[3,4,5]]]))\n",
    "y=tf.constant(([[2.2,3,4],[9,8,7],[1,2,3]]))\n",
    "z=tf.math.multiply(x[:],y)\n",
    "v=y+y\n",
    "w=tf.reduce_sum(z,0)\n",
    "with tf.Session() as sess:  print(x.eval(),\"\\n y \\n\",y.eval(),\"\\n z \\n\",z.eval(),\"\\n w \\n\",w.eval(),\"v\",v.eval())\n",
    "layer=GraphGRU(25,15)\n",
    "hm1=tf.random.normal([25])\n",
    "edges=tf.random.normal([15,15])\n",
    "nodes=tf.random.normal([15])\n",
    "inputs=[hm1,edges,nodes]\n",
    "x1=layer(inputs)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs[0], self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(MyDenseLayer, self).__init__()\n",
    "    self.num_outputs = num_outputs\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_weight(\"kernel\",\n",
    "                                  shape=[int(input_shape[-1]),\n",
    "                                         self.num_outputs])\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.matmul(input, self.kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def build_model(train_x,train_y,test_x,test_y,timesteps,ndays_in,features,epochs,neurons):\n",
    "    print('train_x.shape ',train_x.shape,'train_y.shape ',train_y.shape)\n",
    "    verbose, batch_size = 1, 80\n",
    "# reshape train_x and train_y into [samples, timesteps, features] - ***this is trivial***\n",
    "    train_x=train_x.reshape((train_x.shape[0],ndays_in,features)) # train_x.reshape((train_x.shape[0],train_x.shape[1],1))\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "    test_x=test_x.reshape((test_x.shape[0],ndays_in,features)) # test_x.reshape((train_x.shape[0],train_x.shape[1],1))\n",
    "    test_y = test_y.reshape((test_y.shape[0], test_y.shape[1], 1))\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    print('train_y.shape ',train_y.shape,' train_x.shape ',train_x.shape)\n",
    "        \n",
    "# define model\n",
    "#    neurons=400\n",
    "    denseL=200\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(RepeatVector(n_outputs)) # this makes n_outputs (rather than 1, as in Dense(1))\n",
    "    model.add(LSTM(neurons, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(denseL, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "#    opt=keras.optimizers.Adam(lr=0.005)\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "# save inital weioghts\n",
    "    model.save_weights('modelW.h5')\n",
    "# fit network\n",
    "#    history=model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, \\\n",
    "#                     callbacks=[MyCustomCallback()])\n",
    "\n",
    "\n",
    "    history=model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size, verbose=verbose, \\\n",
    "                     validation_data=(test_x,test_y))\n",
    "#        ilayer=-1\n",
    "#        for layer in model.layers:\n",
    "#            weights=layer.get_weights()\n",
    "#            biases=layer.get_weights()[1]\n",
    "#            print(' layer \\n',layer,' weights.shape ',weights.shape)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MStopa\\AppData\\Local\\Continuum\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "['my_dense_layer_1/kernel:0']\n"
     ]
    }
   ],
   "source": [
    "layer = MyDenseLayer(10)\n",
    "_ = layer(tf.zeros([10, 5])) # Calling the layer `.builds` it.\n",
    "print([var.name for var in layer.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MStopa\\AppData\\Local\\Continuum\\anaconda3\\envs\\PythonGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Tensor(\"linear_1/add:0\", shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "x2 = tf.ones((3,3))\n",
    "x3=[x,x2]\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
